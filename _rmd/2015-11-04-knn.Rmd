---
title: "Unsupervised learning with *K*-nearest-neighbours"
comments: yes
date: '2015-10-30'
output: html_document
layout: post
#modified: `r format(Sys.time(), '%Y-%m-%d')`
excerpt: A simple implementation
published: no
status: process
tags:
- R
- k means
- clustering
- unsupervised learning
- machine learning
categories: Rstats
---

```{r,include = FALSE}

#library(checkpoint)
#checkpoint("2015-05-15")

library(knitr)
library(knn)
library(dplyr)

opts_chunk$set(warning = FALSE, message = FALSE)

```

*K* means is one of the simplest algorithms for unsupervised learning.
It works iteratively to cluster together similar values in a dataset of one to many dimensions ($X\in\mathbb{R}^{m \times n}$).
The algorithmic steps are simple, it relies only on arithmetic means, so it's pretty simple to understand, but can also be pretty powerful.
Because it relies on random sampling to initiate the algorithm it can be quite slow however, as there is a need to complete many replications to get a robust result.

*K*-means features as an exercise in [Coursera](http://www.coursera.com)'s machine learning course, which I am working through.
In this post I produce a simple implementation of the *K* means algorithm in R, and because I'm trying to improve my package writing, I wrap it up into a simple package called [knn]().

### The problem

*K*-means can be divided into two steps.
In a two-dimensional example, given a dataset $X\in\mathbb{R}^{m \times 2}$, the first step is to calculate the closest centroid for each training example $x$.
In my implementation centroids $\{1,..,K\}$ can be assigned manually, or are automatically generated by selecting $k$ random training examples, and setting these as the centroids.

Mathematically, for each training example $x^{(i)}$ we are minimising

$$
||x^{i}-\mu_j||^{2}
$$

i.e. calculating the Euclidean norm of the vector $x^{(i)}$.
This is very simply achieved through pythagorean trigonometry, and is already implemented in R with `norm`, (you need to pass it a matrix rather than a vector).

#### Generate dummy data

The first step is to generate dummy two dimensional data which will demonstrate the principles well.
It's expedient to functionalise this.

```{r}

dummy_group <- function(x = 30, mean = 10, sd = 2) {

  cbind(
    rnorm(x, mean, sd),
    rnorm(x, mean, sd)
  )

}

X <- rbind(
  dummy_group(mean = 10),
  dummy_group(mean = 20),
  dummy_group(mean = 30)
)

```

Which gives us a small dataset with some clear clusters to work with.

```{r,2015-10-30-plot-clusters}

plot(X)

```

#### Calculating norms

Calculating norms can be done with base R using `norm`.

```{r}
norm(as.matrix(c(2,4)),type = "f")
```

The `norm` function isn't too useful in the present case, as we really want to vectorise this process, so I use my own function `rowNorms`:

```{r}

## Define rowNorms()

rowNorms <- function(x) {

norms <- sqrt(rowSums(x ^ 2))
return(norms)

}


```

`rowNorms`  works across a matrix, and will return a vector of norms.

```{r}

## Compare norm ()

norm(cbind(X), type = "f")

## with rowNorms()

rowNorms(X)

```

This function is implemented in [knn]().

#### Calculate distances

So now I can produce a vector of euclidean norms, I can put this into practice for calculating the distance between points $x^i$ and the initial centroids.

In the first instance I will specify some centroids, although as noted, more usual practice is to randomly initiate.

So to calculate the distance between some pre-specified centroids and $X$:
```{r}

## First create a matrix of initial centroids 

centroids <- rbind(
c(15, 15),
c(25, 15),
c(25, 35)
)

## Create a matrix with m rows, consisting entirely of the first centroid.

first_centroid <- matrix(
  rep(centroids[1,], nrow(X)), 
  ncol = 2, 
  byrow = TRUE
  )

diff <- X - first_centroid

distances <- rowNorms(diff)

distances %>% head

```

Rather than repeating this for all three initial centroids, it's easier to wrap this up into a [function]().

Here `knn::find_group()` calculates the closest centroid and returns this as a vector of values $\{1,..,K\}$, where $K$ is the number of centroids.

```{r}

## Note to install knn and make this function work, you would need to do:
## devtools::install_github("ivyleavedtoadflax/knn")

groups <- find_group(X, centroids = centroids)

## Returns a vector of groups based on proximity to the initial centroids

groups %>% head

```

#### Calculate centroids

The next step of the process is to now calculate the new centroid from the groups that were established in the previous step.

Again, this is qute a simple thing to code. 
I achieved it by sequentially subsetting $X$ by $k$, and outputting the new centroids in a matrix  $Y\in\mathbb{R}^{K \times n}$.

```{r}

Y <- centroid_mean(X, groups)

```

With the centroids calculated, now is a good chance to plot again, and check the sanity of the new centroids.

I've rolled plotting up into a function `plot_knn()` (it would be nice to roll this up into a plot method one day...).

```{r}

## Start with the original centroids

plot_knn(X, centroids)

## Now with the updated centroids

plot_knn(X, Y)

```

Not bad, so it looks things are moving in the right direction.
And closer still with an additional iteration.

```{r}

groups1 <- find_group(X, centroids = Y)
Y1 <- centroid_mean(X, groups1)
plot_knn(X, Y1)

```

I could keep on going manually, but I have already implemented the whole process in the function `knn()`. So I'll try that instead, taking care to start with the same initial centroids.

```{r}

# Set seed to make results reproducible
set.seed(1)

foo <- knn(X, centroids)
plot_knn(X, foo$centroids, foo$groups)
```

Great! So what are the final centroids? 

```{r}
foo$centroids %>% round
```

Pretty close to the originals. 
Of course, I knew the true centroids in advance since I fabricated the data; it wouldn't normally be possible to assess the success of the algorithm so easily.

I'll try again with a more difficult example.
Here I am still relying on three normally distributed clusters about the same centroids, but I specify a much greater standard deviation, so to the human eye, the overlap will be much greater.
I'll specify $x = 100$ so that the algorithm has more data to work with.

```{r}
X <- rbind(
  dummy_group(x = 100, mean = 10, sd = 10),
  dummy_group(x = 100, mean = 20, sd = 10),
  dummy_group(x = 100, mean = 30, sd = 10)
)

plot(X)

```

So it's pretty difficult to pick out clusters with the human eye alone.
Let's see what the algorithm makes of it.

Again, I use the `knn()` function which takes the leg work out of the programming. 

In the example below I also set a seed which will make the results reproducible.
As I noted in the pre-amble, the solutions from *K*-means are dependent on the initial centroids, so it is usual practice to repeat the process many times, and then choose the group that has the highest probability.

I've not yet implemented this averaging, so for now I will use `set.seed()` to demonstrate the ideal and not-so-ideal outcomes.

```{r}

## set.seed(2) for a good outcome

set.seed(2)
foo <- knn(X)

plot_knn(X, foo$centroids, foo$groups)

foo$centroids %>% round
```

So we're a little bit out this time after `foo$steps` steps.
I may be stretching the limit of what his very simple implementation can achieve.
Or, perhaps it is to do with how I specified the successful endpoint of, which at present has a very simplistic, and probably inexact implementation.

So what happens if I change the seed?

```{r}

set.seed(100)
bar <- knn(X)
plot_knn(X, bar$centroids, bar$groups)
bar$centroids %>% round(2)
```

So the solution given by the algorithm has changed, and this highlights one of the issues with *K*-means: it can be very dependent on the initial location of centroids.
This is why it is usual to repeat the process many times, then essentially average the results.
I haven't gone that far with my simple implementation, but the in-built implementation in base R (`kmeans`) does include an argument `nstarts` which will give you $n$ possible scenarios.

So how well does `kmeans()` perform in the current scenario? Well not vastly different it turns out.

```{r}
foobar <- kmeans(X, centers = 3, nstart = 1)
plot_knn(X, foobar$centers, foobar$cluster)
foobar$centers %>% round(2)
```

What about the improvement with additional replications?

```{r}
foobar <- kmeans(X, centers = 3, nstart = 1000)
plot_knn(X, foobar$centers, foobar$cluster)
foobar$centers %>% round(2)
```

At two decimal places, there is no discernible difference.
Of course I should note that `kmeans()` uses a more complicated (and more accurate) algorithm than the simplified implementation I have used in [`knn`]().

```{r}
sessionInfo()
```