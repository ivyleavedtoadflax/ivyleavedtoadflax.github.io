---
title: 'Regularised linear regression'
date: '2015-06-22'
#modified: `r format(Sys.time(), '%Y-%m-%d')`
excerpt: "Adding regularisation to vectorised linear regression"
layout: post
published: no
status: draft
comments: true
categories: [Rstats]
tags: [R, machine learning, linear regression, regularisation]
---

```{r,load packages,echo=FALSE,message=FALSE,warning=FALSE}

knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  echo = FALSE
  )

checkpoint::setSnapshot("2015-05-15")
checkpoint::checkpoint("2015-05-15", use.knitr = TRUE)

library(dplyr)
library(magrittr)
library(boot)
library(ucminf)

library(ggplot2)

library(testthat)
library(RColorBrewer)

pal <- brewer.pal(6,"Set1")

```

In this post I reproduce an example similar to an exercise I did for the [coursera](http://www.coursera.org) [MOOC](http://en.wikipedia.org/wiki/Massive_open_online_course) course in machine learning written by Andrew Ng. I'm desperately trying to complete this course, having started it twice now, with the intention of completing all the exercises in R. In the next couple of posts I'm going to complete the equivalent of exercise 5.

The exercise was about creating a vectorised implementation of regularised linear regression, and using this to test some theory relating to the diagnosis of what Ng terms bias (underfitting) and variance (overfitting). Solutions to this exercise area required in matlab, whereas I am working in R, plus I will use a different, but similar dataset to avoid publishing the solutions here.

### Getting some data

In this example I'm using data from the well worn `mtcars` dataset which is included in the datasets package, instead of the data given in the course. FOr the first example presented here, I limit myself to just the first five columnes of this dataset, which are: `mpg`, `cyl`, `disp`, `hp`, and `drat` or: miles per gallon, number of cylinders, displacement ($\text{in}^2$), gross horsepower, and rear axle ratio - 'the ratio between the driveshaft revolutions per minute and the rear axle's revolutions per minute' ([I had to look this up!](www.dodge.com/towing/D/basics/axle_ratio.html); run `?mtcars` for a data description). In this example, I want to predict `mpg` using `cyl`, `disp`, `hp`, and `drat` as features.

Plotting each individually gives us a sense that they all have pretty good linear relationships with `mpg`, but it's also likely that the features are likely to be correlated: e.g. `disp` and `hp`.

```{r 2015-06-22-plot-mtcars}

par(mfrow=c(2,2))

plot(
  mtcars$cyl,
  mtcars$mpg
  )

plot(
  mtcars$disp,
  mtcars$mpg
  )

plot(
  mtcars$hp,
  mtcars$mpg
  )

# plot(
#   mtcars$drat,
#   mtcars$mpg
#   )


```

We can also see that the range of values that the features take vary quite a lot. Whilst `cyl` $\leq{8}$, `disp` tends to measured in the hundred of $\text{in}^2$

```{r}
mtcars[1:4] %>% str

mtcars[1:4] %>% summary

mtcars[1:4] %>% head(n = 3)
```

There are `r nrow(mtcars)` rows in the data set, and for this example I will make a 60/20/20 split on the dataset for train/cross-validate/test.

```{r 2015-06-22-plot-all-data}

set.seed(1337)
orig_data <- mtcars[1:4] %>%
  mutate(
    cyl = ordered(cyl), # should cyl be continuous or categorical?
    n = row_number()
  )

train <- orig_data %>%
  sample_frac(
    .6
  )

test <- orig_data %>%
  filter(
    !n %in% train$n
    )

cross <- test %>%
  sample_frac(
    .5
  )

test <- test %>%
  filter(
    !n %in% cross$n
  )

```

At this point is has already become a little difficult to display all the features on a simple two dimensional plot, so I'll use a combination of colour and shape.

```{r  2015-06-22-plot-all-data1}

orig_data %<>%
  mutate(
    set = ifelse(
      n %in% train$n, 
      "train", 
      ifelse(n %in% cross$n,"cross","test")
    )
  )

p <- orig_data %>% 
  ggplot +
  geom_point(
    aes(
      x = disp,
      y = mpg,
      shape = cyl,
      colour = hp
    ),
    size = 4
  ) +
  xlab(
    expression(Displacement~("in"^2))
  ) +
  ylab(
    expression(mi~gal^-1)
  ) +
  scale_x_continuous(limits=c(50,500)) +
  scale_y_continuous(limits=c(10,35))


p %+% facet_wrap(
  ~set
)

```

### Regularised linear regression

To run the linear regression, I'll build on the vectorised linear regression implementation I implemented [here](http://ivyleavedtoadflax.github.io//rstats/linear_regression/), but this time including a regularisation term.

Regularisation helps us to deal with the problem of overfitting by reducing the weight given to a particular feature $x$. This allows us to retain more features while not giving undue weight to one in particular. Regularisation is mediated by a parameter $\lambda$, as can be seen in the cost function:

$$
J(\theta)=\frac{1}{2m}\Big(\sum^{m}_{i=1}(h_{\theta}(x^{(i)}-y^{(i)})^2\Big)+\frac{\lambda}{2m}\Big(\sum^{n}_{j=1}\theta^2_j\Big)
$$

Since the objective is to minimise $J(\theta)$ (more formally: $\underset{\theta}{\text{min}}J(\theta)$) using a large $\lambda$ will require small values of $\theta_j$ in order to acheive a minima.

A vectorised implementation of the cost function is given below. Note I've used `tcrossprod(theta, X)` as this function was about 1.5 times quicker than the equivalent `X %*% theta` in my tests, and both return the result of $\theta^TX$.

The cost function is not applied to $\theta_0$ as this relates to the intercept parameter.

```{r,echo=TRUE}

J <- function(X, y, theta, lambda) {
  
  m <- length(y)
  
  theta1 <- theta
  
  # Ensure that regularisation is not operating on \theta_0
  
  theta1[1] <- 0
  
  error <- tcrossprod(theta,X)
  error <- as.vector(error) - y
  error1 <- crossprod(error,error)
  
  reg <- (lambda/(2*m)) * crossprod(theta1, theta1)
  
  cost <- (1/(2 * m)) * error1 + reg
  
  return(cost)
  
  }

```

The gradient function is given below, and is the same as that given in my previous post on [regularised logistic regression](./regularised-logistic-regression/) Note that once again the regularisation term excludes $\theta_0$.

$$
\left\{
\begin{array}{ll}  
    \displaystyle\frac{\delta J(\theta)}{\delta\theta_0}=\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j & \text{for}\ j=0 \\
    & \\
    \displaystyle\frac{\delta J(\theta)}{\delta\theta_j}=\left(\frac{1}{m}\sum_{i=1}^n(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j\right) + \frac{\lambda}{m}\theta_j & \text{for}\ j\geq1
\end{array}\right .
$$


```{r,echo=TRUE}

gR <- function(X, y, theta, lambda) {
  
  theta1 <- theta
  theta1[1] <- 0
  
  m <- length(y)
  
  error <- tcrossprod(theta,X)
  error <- as.vector(error) - y
  error <- (1/m) * crossprod(error,X)
  
  reg <- (lambda/(m)) * theta1
  
  delta <- error + reg
  
  return(delta)
  
  }


```

### ucminfisation algorithm

As an optimisation algorithm, I use `ucminf` function from a package of the same name (although you can just use `ucminf` which ships with R as standard). These take the same arguments, so switching out the algorithms in the code is very simple. As default I have stuck to the [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) method, and to $\lambda=0$, i.e. no regularisation.

```{r,echo=TRUE}

X <- cbind(1,train$cyl,train$disp,train$hp)
y <- train$mpg

theta <- rep(1, ncol(X))
lambda <- 0

# Note if using optim, it is good to constrain the optimisation algorithm with 
# method = "BFGS". 
# https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm

ucminf_out <- ucminf(
  par = theta,
  fn = function(t) J(X, y, t, lambda),
  gr = function(t) gR(X, y, t, lambda)
  )


```

So far so good, this seems to work:

```{r}
ucminf_out
```

The output from `ucminf` differs slightly from `ucminf`; here a convergence of $0$ indicates success, and includes the number of times the cost function and gradient functions were called (`$counts`). The output also gives the final cost (`$value`) associated with the parameters (`$par`). For the most part, we are only interested in the parameters themselves, which we can plug into `ggplot` to overlay the model onto the training set.

Let's compare the training data with the new predictions from the model trained on that data.

```{r 2015-06-22-plot-linear-model}

pred <- train %>%
  mutate( 
    mpg = as.vector(tcrossprod(ucminf_out$par, X))
  )

p %+% train + ggtitle("Training")
p %+% pred + ggtitle("Predicted")

```

Based on these simple plots we can see that the simple multiple linear regression model is too simple to represent the curvature we can see in the data.

We can calculate the error related to this model using the sum of the squared differences between the expected and the observed:

$$
J_{train}(\theta)=\frac{1}{2m}\sqrt{\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2}
$$

Note that I added the square root term to put the errors back into the same units as the original measurements, which also makes them much easier to understand. This simple definition does not take into account regularisation, so for the time being, while I am calculating errors, I will set $\lambda = 0$.

```{r,echo=TRUE}

J_train <- function(X, y, theta) {
  
  m <- length(y)
  
  error <- tcrossprod(theta, X)
  error <- as.vector(error) - y
  error <- crossprod(error, error)
  
  # Note that here I have deviated from Andrew Ng by including the sqrt term.
  # Including it reduces the size of the errors to something a little more human
  # readable
  
  return(
      (1/m) * sqrt(error)
  )
  
}

```

Applying this function to the existing model gives a training error of `r sprintf("%.0f",J_train(X, y, ucminf_out$par))`.

## Polynomials 

Looking at the data so far, it's not immediately clear what the best model for the data is. A linear model does the job passably well, but it might be improved by including some non-linearity. We can do this simply by adding polynomials of the input $x$.

Part of the coursera exercise is to create a function to produce polynomials of different features (explanatory variables), but R has a built in polynomial function in the stats package: `poly` which is $50\%$ faster than the equivalent function that I wrote, so I'll stick to the built-in function.

Since we have 3 features, using six degrees of polynomial will result in $`r train %>% select(disp,cyl,hp) %>% mutate(cyl = as.integer(cyl)) %>% as.matrix %>% poly(degree = 6, raw = TRUE) %>% nrow`$ features in total on $`nrow(train)`$ training examples.

```{r}

ucminf_w <- function(X, y, degree = 1, lambda = 0) {
  
  X <- poly(X, degree = degree, raw = TRUE) %>%
    cbind(1,.)

  theta <- rep(1,ncol(X))
  
  ucminf_out <- ucminf(
    par = theta,
    fn = function(t) J(X, y, t, lambda),
    gr = function(t) gR(X, y, t, lambda)
    )
  
  return(
    list(
      con = ucminf_out$convergence,
      par = ucminf_out$par,
      y = data.frame(
        y = tcrossprod(ucminf_out$par, X) %>% as.vector
      )
    )
  )
}

```

```{r,2015-06-22-plot-degree-facet}

# Note that x_0 is not included this time, as I implement this in the function
# itself

X <- cbind(train$cyl,train$disp,train$hp)

# Create dataframe of several degrees to make plotting easier


bla <- train %>% select(-mpg, -n) %>% cbind(ucminf_w(X, y, degree = 1, lambda = 0)$y,.) %>% set_colnames(c("mpg","cyl","disp","hp")) %>% mutate(degree = 1)

for (i in 2:6) {
  
  polynomials <- train %>% select(-mpg, -n) %>% cbind(ucminf_w(X, y, degree = i, lambda = 0)$y,.) %>% set_colnames(c("mpg","cyl","disp","hp")) %>% mutate(degree = i)
  
  bla %<>% rbind_list(
    polynomials
  )
  
}

bla

# Plot df

# p %+% 
#   bla + 
#   geom_line(
#     data = bla %>% mutate(degree = factor(degree)),
#     aes(
#       x = train$disp,
#       y = y,
#       colour = degree,
#       group = degree
#     )
#   ) +
#   facet_wrap(
#     ~degree,
#     scale = "free"
#   )

```

It's not immediately clear which model provides the best fit without looking at the errors, but you can see straight away that the higher the degree of polynomial included, the more jagged the line, and the more it tends to conform to individual points.
This is likely to cause a problem when we apply the models trained here on the cross-validation dataset, as it won't generalise so well as a smoother model.

```{r}

train_error <- sapply(1:6, function(x) {
  
  try({
    
    train_out <- ucminf_w(X = X, y = train$mpg, degree = x,lambda = 0)
    
    theta <- train_out$par
    X <- cbind(1,train$cyl,train$disp,train$hp)
    y <- train$mpg
    
    out <- J_train(X,y,theta)
    
    round(out)
    
  })
})

cross_error <- sapply(1:6, function(x) {
  
  try({
    
    theta <- ucminf_w(X = train$Girth, y = train$Height, degree = x, lambda = 0)$par
    X <- cross$Girth
    y <- cross$Height
    
    out <- J_train(X,y,theta)
    
    round(out)
    
  })
})

theta <- ucminf_w(X = train$Girth, y = train$Height, degree = x, lambda = 0)$par
  X <- cross$Girth
  y <- cross$Height

  tcrossprod(theta,X)
  
```

As it turns out, the model which includes the fourth order polynomial features offers the best fit for the data.

```{r}

data.frame(
  degree = 1:9,
  train = train_error
)

```

So what happens when we apply these models to the cross-validation set?

```{r,2015-06-22-plot-cv-degree-facet}

# Create dataframe of several degrees to make plotting easier

theta <- ucminf_w(train$Girth, train$Height, 1)$par
X <- cbind(1, cross$Girth)

bla <- data.frame(
  x = cross$Girth,
  y = c(tcrossprod(theta, X)),
  degree = 1
)

for (i in 2:6) {
  
  theta <- ucminf_w(train$Girth, train$Height, i)$par
  X <- cbind(1, poly(cross$Girth, raw = TRUE, degree = i))
  
  bla1 <- data.frame(
    x = cross$Girth,
    y = c(tcrossprod(theta, X)),
    degree = i
  )
  
  bla %<>% rbind_list(
    bla1
  )
  
}

# Plot df

p %+% 
  cross +
  geom_line(
    data = bla %>% mutate(degree = factor(degree)),
    aes(
      x = x,
      y = y,
      colour = degree,
      group = degree
    )
  ) +
  facet_wrap(
    ~degree
    )

```

Again, difficult to tell exactly which model is performing the best until we calulate the errors:

```{r}

data.frame(
  degree = 1:9,
  cross = cross_error
)

```

So, once again, the fourth order polynomial model provides the best fit for the data.

That's all for now. Next time I will look at the effect that $\lambda$, the regularisation parameter can have on the models, and explore further the diagnosis of variance and bias.

```{r}
sessionInfo()
```
