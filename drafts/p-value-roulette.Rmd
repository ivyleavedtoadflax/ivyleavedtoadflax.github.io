---
title: "How (not) to make a fool of yourself with p-values"
date: 2015-04-26
#modified: 2015-05-10
excerpt: "What's the real false discovery rate, if it isn't alpha?"
layout: post
published: false
status: draft
comments: true
tags: [p values, R, statistics, research]
---
 

 
 
More carbon is stored in the soil than all the plants and trees that grow upon it, and so correctly assessing the impact of land use changes on soil carbon is essential to our understanding of greenhouse gas balances.
Part of my research has been to do with measuring soil carbon changes after planting trees, and it is generally supposed that planting trees will put more carbon into the soil.
 
These measurements are difficult because soil carbon is incredibly variable, but the absolute changes in the percentage of carbon are very small.
Thus, especially when you start sampling to any depth, you are likely to suffer from low experimental power i.e. an increased chance of concluding that there is no change, when in fact there is one.
 
But in some respects, low power, and wrongfully saying that there is no effect when there is one, is the least of our worries.
It's much more concerning when we say that there is a difference when in fact there is not one.
Traditionally we conduct our tests at $\alpha=0.05$ and $\beta=0.2$, i.e. the probability falsely concluding a difference when there is not one is $5\%$, and the probability of falsely asserting that there is no difference when there is one is $20\%$.
 
When I speak to other researchers and PhD students, everyone has heard of $\alpha$, and everyone thinks they know what a *p*-value is (small: good, big: bad, right?), but almost nobody seems to know about $\beta$, and the implications of low power, despite this being the other side of the $\alpha$ coin[^1]. 
 
[^1]: If you find yourself in this boat, these refs should give you a good grounding: 1) Cohen, J. (1962) “The statistical power of abnormal-social psychological research: a review.”, *Journal of abnormal and social psychology*, 65(3), pp. 145–153. 2) Cohen, J. (1992) “A power primer”, *Psychological bulletin*, 112(1), pp. 155–159. 3) Hoenig, J.M. and Heisey, D.M. (2001) “The Abuse of Power : The Pervasive Fallacy of Power Calculations for Data Analysis”, *The American Statistician*, 55(1), pp. 19–24. 4) Thomas, L. (1997) “Retrospective power analysis: when?”, Conservation Biology, 11(1), pp. 276–280. 5) Thomas, L. and Krebs, C. (1997) “A review of statistical power analysis software”, *Bulletin of the Ecological Society of America*, 78(2), pp. 128–139.
 
What is most concerning of all, is that *p*-values aren't the end of the story in detecting false positives. This problem is explored by David Colquhoun in his 2014 paper 'An investigation of the false discovery rate and the misinterpretation of P values'[^2] (with included R code[^3]), which is what I write about here, whilst trying very hard not to make a fool out of myself.
 
[^2]: Colquhoun, D. (2014) “An investigation of the false discovery rate and the misinterpretation of P values”, *Royal Society Open Science*, 1(140216), pp. 1–15. Available at: 10.1098/rsos.140216 (Accessed: March 23, 2015).
[^3]: Colquhoun D. 2014 Files for running simulated t tests Instructions: [http://www.dcscience.net/](http://www.dcscience.net/) Downloaded from [http://rsos.royalsocietypublishing.org/](http://rsos.royalsocietypublishing.org/) on March 23, 2015; Rscript: [http://www.dcscience.net/files/two_sample-simulation.R](http://www.dcscience.net/files/ two_sample-simulation.R); Excel file with results used in paper: [http://www.dcscience.net/files/t-test-simulations.xlsx](http://www.dcscience.net/files/t-test-simulations.xlsx).
 
### A *p*-value recap
 
So what exactly is the *p*-value? This is not the interesting bit, so I'll talk about this in brief, as there are much more eloquent explanations all over the internet. I'll save time by adapting some code from Kristoffer Magnusson's blog[^4] [RPsychologist.com](http://RPsychologist.com).
 
[^4]:Magnusson, K. (2013) Creating a typical textbook illustration of statistical power using either ggplot or base graphics, RPsychologist.com Available at: [http://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics](http://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics) (Accessed: May 27, 2014). I've not reproduced the code here, but as ever the full code behind this page is available on [github](https://github.com/ivyleavedtoadflax/ivyleavedtoadflax.github.io/tree/master/_rmd).
 
So imagine a classical hypothesis test.
We test treatment *A* against treatment *B*.
We want to know if there is a difference in some measurement between two treatments.
Running a hypothesis test, we can summarise our expected outcomes in terms of the null hypothesis ($H_0$) as follows:
 
|            |$H_0$ True    |$H_0$ False  |
|------------|--------------|-------------|
|Accept $H_0$|No error      |Type II Error|
|            |True Positive |False Negative|  
|Reject $H_0$|Type I Error  |No error     |
|            |False Positive|True Negative| 
 
 
We can define our null ($H_0$) and alternative hypotheses ($H_a$) as follows:
 
$$
\left\{
\begin{array}{ll}  
    H_0: & \mu_0 = 0\\
    H_a:  & \mu_0 \neq 0
\end{array}\right .
$$
 
And this can be presented in a graphical form:
 
![plot of chunk unnamed-chunk-2](/figures/unnamed-chunk-2-1.png) 
 
The left hand curve represents the theoretical sampling distribution if $H_0$ ($\mu=0$) is true, while the right hand curve represents the sampling distribution if $H_a$ is true, more specifically: if $\mu=3.5$.
So these curves show us what happens if we were to conduct the same experiment many times if either of our two hypotheses are true, and the mean of any one individual experiment ($\bar{x}$) would lie somewhere on these two curves.
 
If the value of $\bar{x}$ arrives further to the right than the critical value (denoted by the red line), we will always assume that $H_a$ is true, but there is a $2.5\%$ ($\alpha/2$) chance that we are wrong, and our sample mean $\bar{x}$ really belongs to the $H_0$ curve, i.e. the value is within the blue polygon; hence a Type I error - a false discovery.
So this is where the *p*-value comes in.
If $p<0.05$, then $\bar{x}$ has occured within the rejection zone $\alpha$.
 
Conversely, $\bar{x}$ might arrive at a value lower than (to the left of) the critical value, and we would assume that this is part of the curve $H_0$.
But we could be wrong - the value might belong to the curve $H_a$ in the area denoted by $\beta$, in which case we commit a Type II error - a false negative.
 
Importantly, we can only make these conclusions if only chance were operating, i.e. *p*-values only tell us about a situation where there is nothing going on, and that there is no actual effect.
 
### So why is there a problem?
 
So the issue that David Colquhoun raises in his 2014 paper is that we don't know *a priori* whether $H_0$ is true or not.
Hence if we get a 'significant result' we can never be sure if this is because there was a real effect, and we had sufficient power to detect it, or there isn't really an effect and this is one of the $5\%$ of cases where our statistics make the wrong choice.
 
So in fact, the true 'false discovery rate' (as distinct from both the multiple hypothesis testing adjustment, and the Type I error rate) actually can be calculated as a conditional probability following Bayes' theorem:
 
$$
P(A|B)=\frac{P(A)P(B|A)}{P(B)}
$$
 
Where $P(A|B)$ represents the probability of $A$ given $B$.
In our case, this would be:
 
$$
P(H_0\ \text{is false}|H_0\ \text{rejected}) = \frac{k \times 0.8}{k \times 0.8 + 0.05 \times (1-k)}
$$
 
* Where $k$ is the prevalence of $H_0$ being false among a number of tests.
* 0.8 is the power ($1-\beta$) of the test.
* 0.05 is the Type I error rate ($\alpha$)
 
Of course we don't really know the value of $k$, not without doing a large number of experiments, but it is something that we can estimate from the literature.
In this example, we can just try a range of values, which is what David Colquhoun does in his paper[^2].
 
### t-test simulations
 
So let's start by doing the right thing and conducting an *a-priori* power analysis.
Following Colquhoun I'm going to run tests of two samples a mean difference $\delta=1$, and a standard deviation $\sigma=1$, assuming the nominal $\alpha=0.05$ and $\beta=0.8$.
For a real study we would need to inform our choice of $\sigma$ from a pilot study or from literature values, and we might set $\delta$ to be a number of some biological significance. 
 

{% highlight r %}
power.t.test(
  n = NULL, 
  sd = 1,
  delta = 1,
  sig.level = 0.05,
  type = "two.sample",
  power = 0.8
  )$n %>% round
{% endhighlight %}



{% highlight text %}
## [1] 17
{% endhighlight %}
 
So the minimum sample size for each group to maintain $\alpha=0.05$ and $\beta=0.2$ is 17.
Now we create two datasets from which we will randomly sample for running our simulations.
 

{% highlight r %}
myy1 <- rnorm(1:1000, mean = 0, sd = 1)
myy2 <- rnorm(1:1000, mean = 1, sd = 1)
 
 
 
t_matrix <- matrix(
  ncol = 6,
  nrow = 1000
  )
 
for (i in 1:1000) {
  #simulate 2D data
  
  sampleA <- rnorm(16, mean = 0, sd = 1)
  sampleB <- rnorm(16, mean = 1, sd = 1)
  
  myresult <- t.test(
    sampleA, 
    sampleB, 
    alternative = "two.sided", 
    paired = FALSE, 
    var.equal = TRUE, 
    conf.level = 0.95
    )
  
  t_matrix[i,] <- c(
    t = myresult$statistic,
    p = myresult$p.value,
    up_ci = myresult$conf.int[1],
    lw_ci = myresult$conf.int[2],
    myresult$estimate[1],
    myresult$estimate[2]    
    )
  
  }
 
t_matrix %<>% 
  as.data.frame %>%
  set_colnames(
    c("t","p","up_ci","lw_ci","mean_x","mean_y")
    )
 
 
(t_matrix$p > 0.05) %>% sum
{% endhighlight %}



{% highlight text %}
## [1] 218
{% endhighlight %}



{% highlight r %}
plot(t_matrix$p,type="h")
{% endhighlight %}

![plot of chunk unnamed-chunk-4](/figures/unnamed-chunk-4-1.png) 

{% highlight r %}
plot(t_matrix$mean_y-t_matrix$mean_x,type = "h")
{% endhighlight %}

![plot of chunk unnamed-chunk-4](/figures/unnamed-chunk-4-2.png) 
 

{% highlight r %}
t_matrix1 <- matrix(
  ncol = 6,
  nrow = 1000
  )
 
for (i in 1:1000) {
  #simulate 2D data
  
  sampleA <- rnorm(16, mean = 0, sd = 1)
  sampleB <- rnorm(16, mean = 0, sd = 1)
  
  myresult <- t.test(
    sampleA, 
    sampleB, 
    alternative = "two.sided", 
    paired = FALSE, 
    var.equal = TRUE, 
    conf.level = 0.95
    )
  
  t_matrix1[i,] <- c(
    t = myresult$statistic,
    p = myresult$p.value,
    up_ci = myresult$conf.int[1],
    lw_ci = myresult$conf.int[2],
    myresult$estimate[1],
    myresult$estimate[2]    
    )
  
  }
 
t_matrix1 %<>% 
  as.data.frame %>%
  set_colnames(
    c("t","p","up_ci","lw_ci","mean_x","mean_y")
    )
 
 
(t_matrix1$p > 0.05) %>% sum
{% endhighlight %}



{% highlight text %}
## [1] 946
{% endhighlight %}



{% highlight r %}
plot(t_matrix1$p,type="h")
{% endhighlight %}

![plot of chunk unnamed-chunk-5](/figures/unnamed-chunk-5-1.png) 

{% highlight r %}
plot(t_matrix1$mean_y-t_matrix1$mean_x,type = "h")
{% endhighlight %}

![plot of chunk unnamed-chunk-5](/figures/unnamed-chunk-5-2.png) 

{% highlight r %}
true_neg <- (t_matrix$p < 0.05) %>% sum
false_neg <- (t_matrix$p > 0.05) %>% sum
 
true_pos <- (t_matrix1$p > 0.05) %>% sum
false_pos <- (t_matrix1$p < 0.05) %>% sum
{% endhighlight %}
 
782
946
 
 
218
54
 

{% highlight r %}
k <- 0.1
 
 
((1-k) * false_pos) / sum(
  (1-k) * false_pos,
  k * true_pos
  )
{% endhighlight %}



{% highlight text %}
## [1] 0.3393855
{% endhighlight %}
 
So 
 
$$
P(H_0\ \text{is false}|H_0\ \text{rejected}) = \frac{k \times 0.8}{k \times 0.8 + 0.05 \times (1-k)}
$$
 
 

{% highlight r %}
sessionInfo()
{% endhighlight %}



{% highlight text %}
## R version 3.1.3 (2015-03-09)
## Platform: x86_64-unknown-linux-gnu (64-bit)
## Running under: Ubuntu 14.04.2 LTS
## 
## locale:
##  [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              
##  [3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8    
##  [5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8   
##  [7] LC_PAPER=en_GB.UTF-8       LC_NAME=C                 
##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
## [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C       
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
## [1] magrittr_1.5   xtable_1.7-4   dplyr_0.4.1    testthat_0.9.1
## [5] knitr_1.9     
## 
## loaded via a namespace (and not attached):
## [1] assertthat_0.1  DBI_0.3.1       evaluate_0.5.5  formatR_1.0    
## [5] lazyeval_0.1.10 parallel_3.1.3  Rcpp_0.11.4     stringr_0.6.2  
## [9] tools_3.1.3
{% endhighlight %}
 
