---
title: 'QR decomposition'
date: '2015-08-09'
#modified: `r format(Sys.time(), '%Y-%m-%d')`
excerpt: "and why you should care about it"
layout: post
published: no
status: draft
comments: true
categories: [Rstats]
tags: [R, machine learning, linear regression, regularisation]
---


```{r,load packages,echo=FALSE,message=FALSE,warning=FALSE}

knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  echo = TRUE
  )

#checkpoint::checkpoint("2015-08-01")
checkpoint::checkpoint("2015-08-01", use.knitr = TRUE)

library(dplyr)
library(testthat)
#library(magrittr)
#library(boot)
#library(ucminf)
#library(ggplot2)
#library(tidyr)
#library(MASS)
#library(RColorBrewer)

```

In my last [post](../R_classes/) I created my own package for linear regression which relied on an optimisation algorithm to get the orindary least squares (OLS) estimate. The standard approach to a linear regression problem is to solve the normal equation given a design matrix $X$, and response vector $\vec{y}$:

$$\theta=(X^TX)^{-1}X^T\vec{y}$$

Whilst calculating the regression coefficient $\theta$ using the normal equation gives *the* solution to the problem, according Andrew Ng in the coursera Machine Learning course, an implementation that relies on an optimisation would tend to perform better if the number of features is very large ($n>10^6$), this is because of the requiment for the computing of $(X^TX)^{-1}$, which is an $n \times n$ dimentional matrix ($\in\mathbb{R}^{n \times n}$).

In writing my package [vlrr](https://github.com/ivyleavedtoadflax/vlrr), I drew heavily on this [guide](https://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf) which recommends not attempting to compute $\theta$ using the normal equation directly, but instead uses a QR decomposition (implemented in R in the `qr()` function). This piqued my interest; what is this mystical QR decomposition, and why is it better to implement linear regression using this method?

A quick Google, and you will see that QR decomposition has been considered by some to be one of the [most influential algorithms of the 20th Century](http://www.siam.org/pdf/news/637.pdf). In this post, and the on ethat follows, I document my attempt to understand exactly what the QR decomposition is, and why it is so useful for statistical techniques.

### What is QR decomposition?

In simple terms[^1], a QR decomposition is the breaking down of a matrix into two product matrices with specific properties. If we start with a matrix $M$, QR decomposition would give us $M=QR$ where $Q$ is an orthogonal matrix, and $R$ an upper triangular matrix.

[^1]: [https://en.wikipedia.org/wiki/QR_decomposition](https://en.wikipedia.org/wiki/QR_decomposition)

A bit rusty on your linear algebra? Me too. So an orthogonal matrix is an invertible matrix for which $Q^T=Q^{-1}$. This is a useful property, as it is much easier to calculate the transpose of a matrix than its inverse. To give an example[^2], $Q$ is an orthogonal matrix:

[^2]: [http://mathworld.wolfram.com/OrthogonalMatrix.html](http://mathworld.wolfram.com/OrthogonalMatrix.html)

$$
Q=\frac{1}{3}
\begin{bmatrix}
2 & -2 & 1 \\
1 & 2 & 2 \\
2 & 1 & -2 \\
\end{bmatrix}
$$

so that:

$$
Q^T=Q^{-1}=\begin{bmatrix}
0.6\dot{6} & -0.6\dot{6} & 0.3\dot{3} \\
0.3\dot{3} & 0.6\dot{6} & 0.6\dot{6} \\
0.6\dot{6} & 0.3\dot{3} & -0.6\dot{6} \\
\end{bmatrix}
$$

And using `rbenchmark` we can see the computational advantage of being able to transpose instead of invert an orthogonal matrix:

```{r,echo=FALSE}

library(rbenchmark)

Q <- 1/3 * matrix(c(2,1,2,-2,2,1,1,2,-2),ncol=3)

# check that the three methods are actually equivalent before performing
# benchmark

expect_equal(
  t(Q),
  solve(Q),
  MASS::ginv(Q)
)

benchmark(
  t = t(Q),
  solve = solve(Q),
  ginv = MASS::ginv(Q),
  columns = c("test", "replications", "elapsed", "relative"),
  replications = 10000
  )

```

So what about an upper-triangular matrix? This is simply a square matrix with a diagonal division from \{1,1\} to \{m,n\}, below which all values are $0$; e.g.:

$$
R=\begin{bmatrix}
a_{1,1} & a_{1,2} & a_{1,3} & \cdots & a_{1,n} \\
 & a_{2,2,} & a_{2,3} & \cdots &a_{2,n} \\
 &  & a_{3,3} & \cdots & a_{3,n} \\
 &  &  & \ddots & \vdots\\
0 &  &  &  & a_{m,n}
\end{bmatrix}
$$

So, to give an example of a $QR$ decomposition using R, before launching into the process of how to do it:




The process of determining $Q$ and $R$ without a computer is lengthy, but straightforward. For my own memory if nothing else, I will describe the process here:

### The Gram-Schmidt process

The first step in calculating $QR$, is to calculate $Q$. This follows a process called the Gram-Schmidt process. For the sake of recapping my naming conventions, I am trying to compute:

$$
M=QR
$$

or:

$$
\begin{bmatrix}
m_{1,1} & m_{1,2} & \cdots & m_{1,n} \\
m_{2,1} & m_{2,2} & \cdots & m_{2,n} \\
m_{3,1} & m_{3,2} & \cdots & m_{3,n} \\
\vdots & \vdots & \ddots & \vdots \\
m_{m,1} & m_{m,2} & \cdots & m_{m,n}
\end{bmatrix} = 
\begin{bmatrix}
u_{1,1} & u_{1,2} & \cdots & u_{1,n} \\
u_{2,1} & u_{2,2} & \cdots & u_{2,n} \\
u_{3,1} & u_{3,2} & \cdots & u_{3,n} \\
\vdots & \vdots & \ddots& \vdots \\
u_{m,1} & u_{m,2} & \cdots & u_{m,n}
\end{bmatrix}
\begin{bmatrix}
a_{1,1} & a_{1,2} & a_{1,3} & \cdots & a_{1,n} \\
 & a_{2,2,} & a_{2,3} & \cdots &a_{2,n} \\
 &  & a_{3,3} & \cdots & a_{3,n} \\
 &  &  & \ddots & \vdots\\
0 &  &  &  & a_{m,n}
\end{bmatrix}
$$



```{r,eval=FALSE}
qx <- qr(x)
coef <- solve.qr(qx, y)
```
